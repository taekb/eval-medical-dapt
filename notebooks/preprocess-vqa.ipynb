{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing Steps for VQA-RAD, PathVQA, and SLAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from llava.constants import DEFAULT_IMAGE_TOKEN\n",
    "\n",
    "# Load LLaVA-Med alignment and instruction datasets for reference\n",
    "llava_med_dir = '../../llava-med'\n",
    "\n",
    "llava_med_align_dir = osp.join(llava_med_dir, 'data/alignment')\n",
    "llava_med_align_data = json.load(open(osp.join(llava_med_align_dir, 'llava_med_alignment_500k.json')))\n",
    "\n",
    "llava_med_instruct_dir = osp.join(llava_med_dir, 'data/instruct')\n",
    "llava_med_instruct_data = json.load(open(osp.join(llava_med_instruct_dir, 'llava_med_instruct_10k.json')))\n",
    "\n",
    "# VQA-RAD dataset\n",
    "vqa_rad_dir = '/data/vqa-rad/'\n",
    "vqa_rad_json = json.load(open(osp.join(vqa_rad_dir, 'VQA_RAD Dataset Public.json'))) # List of dictionaries\n",
    "vqa_rad_df = pd.read_json(osp.join(vqa_rad_dir, 'VQA_RAD Dataset Public.json')) # Dataframe format\n",
    "vqa_rad_df['answer'] = vqa_rad_df['answer'].apply(lambda x: str(x).lower().strip())\n",
    "vqa_rad_df['answer_type'] = vqa_rad_df['answer_type'].apply(lambda x: str(x).lower().strip())\n",
    "vqa_rad_image_dir = osp.join(vqa_rad_dir, 'images')\n",
    "vqa_rad_images = os.listdir(vqa_rad_image_dir)\n",
    "\n",
    "# PathVQA dataset\n",
    "pvqa_dir = '/data/pvqa'\n",
    "pvqa_image_dir = osp.join(pvqa_dir, 'images')\n",
    "pvqa_train_images = [osp.join(pvqa_image_dir, f'train/{i}') for i in os.listdir(osp.join(pvqa_image_dir, 'train'))]\n",
    "pvqa_val_images = [osp.join(pvqa_image_dir, f'val/{i}') for i in os.listdir(osp.join(pvqa_image_dir, 'val'))]\n",
    "pvqa_test_images = [osp.join(pvqa_image_dir, f'test/{i}') for i in os.listdir(osp.join(pvqa_image_dir, 'test'))]\n",
    "\n",
    "# SLAKE dataset\n",
    "slake_dir = '/data/slake'\n",
    "slake_image_dir = osp.join(slake_dir, 'imgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format used by LLaVA-Med for medical concept alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '32357477_ijms-21-03049-f006',\n",
       " 'image': '32357477_ijms-21-03049-f006.jpg',\n",
       " 'conversatons': [{'from': 'human',\n",
       "   'value': 'Illustrate the image through a descriptive explanation\\n<image>'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Determination of metaphase II entry in oocytes pre-exposed to ferrocenyl 4-(alkylamino)-1,4-dihydroquinolines. After incubation or not with compounds 9, 6, 10, 7, 11, 8 for 24 h, oocytes were rinsed four times in ND96 for 30 min, before progesterone stimulation. White spot appearance was scored after 15 h. N refers to the number of females and n to the number of oocytes (N = 2 and n = 60).'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_med_align_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format used by LLaVA-Med for visual instruction tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '17506892_F1',\n",
       " 'image': '17506892_F1.jpg',\n",
       " 'conversatons': [{'from': 'human',\n",
       "   'value': 'Can you describe the image for me?\\n<image>'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The image consists of maps of significant voxels representing regions of hypoperfusion in FTLD patients according to their clinical diagnosis. These maps are superimposed onto a reference T1-weighted MRI image. There are five rows, each representing a different patient subgroup: bvFTD, SD, PNFA, PSP, and CBD. The neurological convention is followed, with the left side of the brain on the left side of the image.'},\n",
       "  {'from': 'human', 'value': 'What is the modality of this image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"The modality of this image is a T1-weighted MRI, which is a type of magnetic resonance imaging used to visualize the brain's anatomy.\"},\n",
       "  {'from': 'human',\n",
       "   'value': 'What stands out or draws attention in this image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'What stands out in this image are the regions of hypoperfusion, which are represented by significant voxels. These regions are color-coded and overlaid on the T1-weighted MRI image, highlighting the differences in blood flow between the various patient subgroups.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Can you tell me more about the first row representing bvFTD?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The first row represents the behavioral variant of Frontotemporal Dementia (bvFTD) patient subgroup. In this row, you can see the regions of hypoperfusion specific to bvFTD patients, which are highlighted by significant voxels on the T1-weighted MRI image.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What about the second row for Semantic Dementia (SD)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The second row represents the Semantic Dementia (SD) patient subgroup. In this row, the regions of hypoperfusion specific to SD patients are highlighted by significant voxels on the T1-weighted MRI image, showing the areas of the brain affected by this type of dementia.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Can you describe the third row for Progressive Non-Fluent Aphasia (PNFA)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The third row represents the Progressive Non-Fluent Aphasia (PNFA) patient subgroup. In this row, the regions of hypoperfusion specific to PNFA patients are highlighted by significant voxels on the T1-weighted MRI image. This helps to visualize the brain areas affected by this particular type of aphasia.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What about the fourth row for Progressive Supranuclear Palsy (PSP)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The fourth row represents the Progressive Supranuclear Palsy (PSP) patient subgroup. In this row, the regions of hypoperfusion specific to PSP patients are highlighted by significant voxels on the T1-weighted MRI image. This provides insight into the brain areas impacted by this neurodegenerative disorder.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Finally, can you describe the fifth row for Corticobasal Degeneration (CBD)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The fifth row represents the Corticobasal Degeneration (CBD) patient subgroup. In this row, the regions of hypoperfusion specific to CBD patients are highlighted by significant voxels on the T1-weighted MRI image. This helps to visualize the brain areas affected by this rare neurodegenerative disease.'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_med_instruct_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQA-RAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>phrase_type</th>\n",
       "      <th>qid_linked_id</th>\n",
       "      <th>image_case_url</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_organ</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>question</th>\n",
       "      <th>question_rephrase</th>\n",
       "      <th>question_relation</th>\n",
       "      <th>question_frame</th>\n",
       "      <th>question_type</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>freeform</td>\n",
       "      <td>03f451ca-de62-4617-9679-e836026a7642</td>\n",
       "      <td>https://medpix.nlm.nih.gov/case?id=48e1dd0e-85...</td>\n",
       "      <td>synpic54610.jpg</td>\n",
       "      <td>HEAD</td>\n",
       "      <td>not evaluated</td>\n",
       "      <td>Are regions of the brain infarcted?</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>PRES</td>\n",
       "      <td>yes</td>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>freeform</td>\n",
       "      <td>06e26b2c-04b9-42bc-8e98-1de30a0f7682</td>\n",
       "      <td>https://medpix.nlm.nih.gov/case?id=b197277b-69...</td>\n",
       "      <td>synpic29265.jpg</td>\n",
       "      <td>CHEST</td>\n",
       "      <td>not evaluated</td>\n",
       "      <td>Are the lungs normal appearing?</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>ABN</td>\n",
       "      <td>no</td>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>freeform</td>\n",
       "      <td>0d0e8b6b-7753-4788-9b6d-dc7f25250c3f</td>\n",
       "      <td>https://medpix.nlm.nih.gov/case?id=b197277b-69...</td>\n",
       "      <td>synpic29265.jpg</td>\n",
       "      <td>CHEST</td>\n",
       "      <td>not evaluated</td>\n",
       "      <td>Is there evidence of a pneumothorax</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>PRES</td>\n",
       "      <td>no</td>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>freeform</td>\n",
       "      <td>0e90b6bc-265f-490b-a039-509b9907a3cb</td>\n",
       "      <td>https://medpix.nlm.nih.gov/case?id=19aa8a2b-35...</td>\n",
       "      <td>synpic28602.jpg</td>\n",
       "      <td>CHEST</td>\n",
       "      <td>given</td>\n",
       "      <td>What type of imaging does this not represent?</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>MODALITY</td>\n",
       "      <td>ultrasound</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>freeform</td>\n",
       "      <td>1179f612-12e0-4dda-aee0-f14a5200be7b</td>\n",
       "      <td>https://medpix.nlm.nih.gov/case?id=b197277b-69...</td>\n",
       "      <td>synpic29265.jpg</td>\n",
       "      <td>CHEST</td>\n",
       "      <td>given</td>\n",
       "      <td>Is this a MRI of the chest?</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>MODALITY</td>\n",
       "      <td>no</td>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid phrase_type                         qid_linked_id  \\\n",
       "0    0    freeform  03f451ca-de62-4617-9679-e836026a7642   \n",
       "1    1    freeform  06e26b2c-04b9-42bc-8e98-1de30a0f7682   \n",
       "2    2    freeform  0d0e8b6b-7753-4788-9b6d-dc7f25250c3f   \n",
       "3    3    freeform  0e90b6bc-265f-490b-a039-509b9907a3cb   \n",
       "4    4    freeform  1179f612-12e0-4dda-aee0-f14a5200be7b   \n",
       "\n",
       "                                      image_case_url       image_name  \\\n",
       "0  https://medpix.nlm.nih.gov/case?id=48e1dd0e-85...  synpic54610.jpg   \n",
       "1  https://medpix.nlm.nih.gov/case?id=b197277b-69...  synpic29265.jpg   \n",
       "2  https://medpix.nlm.nih.gov/case?id=b197277b-69...  synpic29265.jpg   \n",
       "3  https://medpix.nlm.nih.gov/case?id=19aa8a2b-35...  synpic28602.jpg   \n",
       "4  https://medpix.nlm.nih.gov/case?id=b197277b-69...  synpic29265.jpg   \n",
       "\n",
       "  image_organ     evaluation                                       question  \\\n",
       "0        HEAD  not evaluated            Are regions of the brain infarcted?   \n",
       "1       CHEST  not evaluated                Are the lungs normal appearing?   \n",
       "2       CHEST  not evaluated            Is there evidence of a pneumothorax   \n",
       "3       CHEST          given  What type of imaging does this not represent?   \n",
       "4       CHEST          given                    Is this a MRI of the chest?   \n",
       "\n",
       "  question_rephrase question_relation question_frame question_type  \\\n",
       "0              NULL              NULL           NULL          PRES   \n",
       "1              NULL              NULL           NULL           ABN   \n",
       "2              NULL              NULL           NULL          PRES   \n",
       "3              NULL              NULL           NULL      MODALITY   \n",
       "4              NULL              NULL           NULL      MODALITY   \n",
       "\n",
       "       answer answer_type  \n",
       "0         yes      closed  \n",
       "1          no      closed  \n",
       "2          no      closed  \n",
       "3  ultrasound        open  \n",
       "4          no      closed  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_rad_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the closed-ended QA pairs for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only closed-ended QA pairs\n",
    "df = vqa_rad_df[(vqa_rad_df['answer_type'] == 'closed')]\n",
    "\n",
    "# Reference: https://huggingface.co/datasets/flaviagiammarino/vqa-rad/blob/main/scripts/processing.py\n",
    "# Retrieve the train and test splits\n",
    "train_df = df[df['phrase_type'].isin(['freeform', 'para'])]\n",
    "test_df = df[df['phrase_type'].isin(['test_freeform', 'test_para'])]\n",
    "\n",
    "# Remove duplicate examples\n",
    "subset_cols = ['image_name', 'question', 'answer']\n",
    "train_df = train_df.drop_duplicates(subset=subset_cols, ignore_index=True)\n",
    "test_df = test_df.drop_duplicates(subset=subset_cols, ignore_index=True)\n",
    "\n",
    "# Remove data leakage\n",
    "train_df = train_df[~train_df[subset_cols].apply(tuple, 1).isin(test_df[subset_cols].apply(tuple, 1))]\n",
    "\n",
    "# Clean up QA formatting \n",
    "f = lambda x: re.sub(' +', ' ', str(x).lower()).replace(\" ?\", \"?\").strip()\n",
    "train_df['question'] = train_df['question'].apply(f)\n",
    "train_df['answer'] = train_df['answer'].apply(f)\n",
    "test_df['question'] = test_df['question'].apply(f)\n",
    "test_df['answer'] = test_df['answer'].apply(f)\n",
    "\n",
    "# Reindex the examples\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# NOTE: For the closed-ended questions whose answer is not yes/no, we manually fix the options in the .jsonl files to follow what the question is asking.\n",
    "def convert_to_llava_instruct_format(df):\n",
    "    converted = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        question = row['question'].capitalize()\n",
    "        answer = row['answer'].lower().strip()\n",
    "\n",
    "        # Build conversation\n",
    "        row_dict = dict(\n",
    "            id=osp.splitext(row['image_name'])[0],\n",
    "            image=row['image_name'],\n",
    "            conversations=[\n",
    "                {'from': 'human', 'value': f'{DEFAULT_IMAGE_TOKEN}\\n{question}'},\n",
    "                {'from': 'gpt', 'value': answer}\n",
    "            ],\n",
    "            options=['yes', 'no']\n",
    "        )\n",
    "        converted.append(row_dict)\n",
    "\n",
    "    return converted\n",
    "\n",
    "# Converted to JSON format\n",
    "# NOTE: Should have 272 QA pairs\n",
    "train_json_orig = convert_to_llava_instruct_format(train_df)\n",
    "test_json_orig = convert_to_llava_instruct_format(test_df)\n",
    "\n",
    "closed_dir = osp.join(vqa_rad_dir, 'closed')\n",
    "os.makedirs(closed_dir, exist_ok=True)\n",
    "\n",
    "def write_jsonl(data, path):\n",
    "    with open(path, 'w') as fh:\n",
    "        for qa in data:\n",
    "            json.dump(qa, fh)\n",
    "            fh.write('\\n')\n",
    "\n",
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as fh:\n",
    "        for line in fh:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data\n",
    "\n",
    "write_jsonl(train_json_orig, osp.join(closed_dir, 'train_orig.jsonl'))\n",
    "write_jsonl(test_json_orig, osp.join(closed_dir, 'test_orig.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'synpic42202',\n",
       " 'image': 'synpic42202.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nIs there evidence of an aortic aneurysm?'},\n",
       "  {'from': 'gpt', 'value': 'yes'}],\n",
       " 'options': ['yes', 'no']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "# Reference: https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md\n",
    "read_jsonl(osp.join(closed_dir, 'test_orig.jsonl'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '18021452_F4',\n",
       " 'image': '18021452_F4.jpg',\n",
       " 'conversatons': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat does the image show in a few words?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The image shows a carcinoid tumor penetrating the subserosa of the small bowel, with H&E staining at 20x magnification.'},\n",
       "  {'from': 'human', 'value': 'What is the modality of this image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The modality of this image is a light microscopy with Hematoxylin and Eosin (H&E) staining.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What are the key or essential components of this image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The key components of this image are the small bowel tissue, the carcinoid tumor, the subserosa layer, and the H&E staining that highlights the cellular structures.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Can you describe the appearance of the carcinoid tumor in the image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The carcinoid tumor appears as an irregular mass with distinct cellular structures, infiltrating the subserosa layer of the small bowel. The H&E staining helps to visualize the tumor cells, which may have a different color or texture compared to the surrounding healthy tissue.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'How does the subserosa layer look in the image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'In the image, the subserosa layer appears as a thin, fibrous layer surrounding the small bowel. The carcinoid tumor can be seen penetrating this layer, indicating its invasive nature.'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with LLaVA-Med instruction data\n",
    "llava_med_instruct_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PathVQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of QA pairs: 32795\n",
      "Total number of closed-ended yes/no questions: 16332\n",
      "Yes: 8993, No: 7339\n"
     ]
    }
   ],
   "source": [
    "# Load the QA files\n",
    "pvqa_train_pkl = pickle.load(open(osp.join(pvqa_dir, 'qas/train/train_qa.pkl'), 'rb'))\n",
    "pvqa_val_pkl = pickle.load(open(osp.join(pvqa_dir, 'qas/val/val_qa.pkl'), 'rb'))\n",
    "pvqa_test_pkl = pickle.load(open(osp.join(pvqa_dir, 'qas/test/test_qa.pkl'), 'rb'))\n",
    "\n",
    "# Convert to DataFrame\n",
    "pvqa_train_df = pd.DataFrame.from_records(pvqa_train_pkl)\n",
    "pvqa_val_df = pd.DataFrame.from_records(pvqa_val_pkl)\n",
    "pvqa_test_df = pd.DataFrame.from_records(pvqa_test_pkl)\n",
    "\n",
    "# Add file format extension to image column\n",
    "pvqa_train_df['image'] = pvqa_train_df['image'].apply(lambda x: f'{x}.jpg')\n",
    "pvqa_val_df['image'] = pvqa_val_df['image'].apply(lambda x: f'{x}.jpg')\n",
    "pvqa_test_df['image'] = pvqa_test_df['image'].apply(lambda x: f'{x}.jpg')\n",
    "\n",
    "# Add new column indicating whether the question is closed- or open-ended\n",
    "# NOTE: The original PathVQA dataset \n",
    "pvqa_train_df['answer_type'] = pvqa_train_df['answer'].apply(\n",
    "    lambda x: 'closed' if (\n",
    "        len(re.findall(r'\\byes\\b|\\bno\\b', x)) > 0 and x.lower().strip() in ['yes', 'no']\n",
    "    ) else 'open'\n",
    ")\n",
    "pvqa_val_df['answer_type'] = pvqa_val_df['answer'].apply(\n",
    "    #lambda x: 'closed' if ('yes' in x.lower() or 'no' in x.lower()) else 'open'\n",
    "    lambda x: 'closed' if (\n",
    "        len(re.findall(r'\\byes\\b|\\bno\\b', x)) > 0 and x.lower().strip() in ['yes', 'no']\n",
    "    ) else 'open'\n",
    ")\n",
    "pvqa_test_df['answer_type'] = pvqa_test_df['answer'].apply(\n",
    "    #lambda x: 'closed' if ('yes' in x.lower() or 'no' in x.lower()) else 'open'\n",
    "    lambda x: 'closed' if (\n",
    "        len(re.findall(r'\\byes\\b|\\bno\\b', x)) > 0 and x.lower().strip() in ['yes', 'no']\n",
    "    ) else 'open'\n",
    ")\n",
    "\n",
    "print('Total number of QA pairs: {}'.format(\n",
    "    pvqa_train_df.shape[0] + pvqa_val_df.shape[0] + pvqa_test_df.shape[0]\n",
    "))\n",
    "print('Total number of closed-ended yes/no questions: {}'.format(\n",
    "    pvqa_train_df[pvqa_train_df['answer_type'] == 'closed'].shape[0] + \\\n",
    "    pvqa_val_df[pvqa_val_df['answer_type'] == 'closed'].shape[0] + \\\n",
    "    pvqa_test_df[pvqa_test_df['answer_type'] == 'closed'].shape[0]\n",
    "))\n",
    "print('Yes: {}, No: {}'.format(\n",
    "    pvqa_train_df[pvqa_train_df['answer'] == 'yes'].shape[0] + \\\n",
    "    pvqa_val_df[pvqa_val_df['answer'] == 'yes'].shape[0] + \\\n",
    "    pvqa_test_df[pvqa_test_df['answer'] == 'yes'].shape[0],\n",
    "    pvqa_train_df[pvqa_train_df['answer'] == 'no'].shape[0] + \\\n",
    "    pvqa_val_df[pvqa_val_df['answer'] == 'no'].shape[0] + \\\n",
    "    pvqa_test_df[pvqa_test_df['answer'] == 'no'].shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only interested in evaluating on closed-ended questions, we filter out all closed-ended questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvqa_train_df = pvqa_train_df[pvqa_train_df['answer_type'] == 'closed'].reset_index(drop=True)\n",
    "pvqa_val_df = pvqa_val_df[pvqa_val_df['answer_type'] == 'closed'].reset_index(drop=True)\n",
    "pvqa_test_df = pvqa_test_df[pvqa_test_df['answer_type'] == 'closed'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we convert each `.pkl` file into a `.json` file with the same format as that of the alignment and VQA-RAD datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_llava_instruct_format(df, q_col='question', a_col='answer', img_col='image_name'):\n",
    "    converted = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        question = row[q_col].capitalize()\n",
    "        answer = row[a_col].lower().strip()\n",
    "\n",
    "        # Build conversation\n",
    "        row_dict = dict(\n",
    "            id=osp.splitext(row[img_col])[0],\n",
    "            image=row[img_col],\n",
    "            conversations=[\n",
    "                {'from': 'human', 'value': f'{DEFAULT_IMAGE_TOKEN}\\n{question}'},\n",
    "                {'from': 'gpt', 'value': answer}\n",
    "            ],\n",
    "            options=['yes', 'no']\n",
    "        )\n",
    "        converted.append(row_dict)\n",
    "\n",
    "    return converted\n",
    "\n",
    "pvqa_train_final = convert_to_llava_instruct_format(pvqa_train_df, img_col='image')\n",
    "pvqa_val_final = convert_to_llava_instruct_format(pvqa_val_df, img_col='image')\n",
    "pvqa_test_final = convert_to_llava_instruct_format(pvqa_test_df, img_col='image')\n",
    "\n",
    "pvqa_closed_dir = osp.join(pvqa_dir, 'closed')\n",
    "os.makedirs(pvqa_closed_dir, exist_ok=True)\n",
    "\n",
    "def write_jsonl(data, path):\n",
    "    with open(path, 'w') as fh:\n",
    "        for qa in data:\n",
    "            json.dump(qa, fh)\n",
    "            fh.write('\\n')\n",
    "\n",
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as fh:\n",
    "        for line in fh:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data\n",
    "\n",
    "write_jsonl(pvqa_train_final, osp.join(pvqa_closed_dir, 'train.jsonl'))\n",
    "write_jsonl(pvqa_val_final, osp.join(pvqa_closed_dir, 'val.jsonl'))\n",
    "write_jsonl(pvqa_test_final, osp.join(pvqa_closed_dir, 'test.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'test_0167',\n",
       " 'image': 'test_0167.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nAre the histone subunits positively charged,  thus allowing the compaction of the negatively charged dna?'},\n",
       "  {'from': 'gpt', 'value': 'yes'}],\n",
       " 'options': ['yes', 'no']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "read_jsonl(osp.join(pvqa_closed_dir, 'test.jsonl'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '17506892_F1',\n",
       " 'image': '17506892_F1.jpg',\n",
       " 'conversatons': [{'from': 'human',\n",
       "   'value': 'Can you describe the image for me?\\n<image>'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The image consists of maps of significant voxels representing regions of hypoperfusion in FTLD patients according to their clinical diagnosis. These maps are superimposed onto a reference T1-weighted MRI image. There are five rows, each representing a different patient subgroup: bvFTD, SD, PNFA, PSP, and CBD. The neurological convention is followed, with the left side of the brain on the left side of the image.'},\n",
       "  {'from': 'human', 'value': 'What is the modality of this image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"The modality of this image is a T1-weighted MRI, which is a type of magnetic resonance imaging used to visualize the brain's anatomy.\"},\n",
       "  {'from': 'human',\n",
       "   'value': 'What stands out or draws attention in this image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'What stands out in this image are the regions of hypoperfusion, which are represented by significant voxels. These regions are color-coded and overlaid on the T1-weighted MRI image, highlighting the differences in blood flow between the various patient subgroups.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Can you tell me more about the first row representing bvFTD?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The first row represents the behavioral variant of Frontotemporal Dementia (bvFTD) patient subgroup. In this row, you can see the regions of hypoperfusion specific to bvFTD patients, which are highlighted by significant voxels on the T1-weighted MRI image.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What about the second row for Semantic Dementia (SD)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The second row represents the Semantic Dementia (SD) patient subgroup. In this row, the regions of hypoperfusion specific to SD patients are highlighted by significant voxels on the T1-weighted MRI image, showing the areas of the brain affected by this type of dementia.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Can you describe the third row for Progressive Non-Fluent Aphasia (PNFA)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The third row represents the Progressive Non-Fluent Aphasia (PNFA) patient subgroup. In this row, the regions of hypoperfusion specific to PNFA patients are highlighted by significant voxels on the T1-weighted MRI image. This helps to visualize the brain areas affected by this particular type of aphasia.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What about the fourth row for Progressive Supranuclear Palsy (PSP)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The fourth row represents the Progressive Supranuclear Palsy (PSP) patient subgroup. In this row, the regions of hypoperfusion specific to PSP patients are highlighted by significant voxels on the T1-weighted MRI image. This provides insight into the brain areas impacted by this neurodegenerative disorder.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Finally, can you describe the fifth row for Corticobasal Degeneration (CBD)?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The fifth row represents the Corticobasal Degeneration (CBD) patient subgroup. In this row, the regions of hypoperfusion specific to CBD patients are highlighted by significant voxels on the T1-weighted MRI image. This helps to visualize the brain areas affected by this rare neurodegenerative disease.'}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_med_instruct_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aftering only selecting closed-ended QA pairs in English:\n",
      "Train: 1943\n",
      "Val: 422\n",
      "Test: 416\n",
      "Total: 2781\n"
     ]
    }
   ],
   "source": [
    "slake_train_df = pd.read_json(osp.join(slake_dir, 'train.json'))\n",
    "slake_val_df = pd.read_json(osp.join(slake_dir, 'validate.json'))\n",
    "slake_test_df = pd.read_json(osp.join(slake_dir, 'test.json'))\n",
    "\n",
    "# Only take the QA pairs in English and closed-ended questions\n",
    "slake_train_df = slake_train_df[(slake_train_df['q_lang'] == 'en') & (slake_train_df['answer_type'] == 'CLOSED')].reset_index(drop=True)\n",
    "slake_val_df = slake_val_df[(slake_val_df['q_lang'] == 'en') & (slake_val_df['answer_type'] == 'CLOSED')].reset_index(drop=True)\n",
    "slake_test_df = slake_test_df[(slake_test_df['q_lang'] == 'en') & (slake_test_df['answer_type'] == 'CLOSED')].reset_index(drop=True)\n",
    "\n",
    "slake_train_df['answer_type'] = slake_train_df['answer_type'].apply(lambda x: x.lower())\n",
    "slake_val_df['answer_type'] = slake_val_df['answer_type'].apply(lambda x: x.lower())\n",
    "slake_test_df['answer_type'] = slake_test_df['answer_type'].apply(lambda x: x.lower())\n",
    "\n",
    "print('Aftering only selecting closed-ended QA pairs in English:')\n",
    "print(f'Train: {slake_train_df.shape[0]}')\n",
    "print(f'Val: {slake_val_df.shape[0]}')\n",
    "print(f'Test: {slake_test_df.shape[0]}')\n",
    "print(f'Total: {slake_train_df.shape[0] + slake_val_df.shape[0] + slake_test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>img_name</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>q_lang</th>\n",
       "      <th>location</th>\n",
       "      <th>modality</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>base_type</th>\n",
       "      <th>content_type</th>\n",
       "      <th>triple</th>\n",
       "      <th>qid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>xmlab1/source.jpg</td>\n",
       "      <td>Does the picture contain liver?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>en</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>MRI</td>\n",
       "      <td>closed</td>\n",
       "      <td>vqa</td>\n",
       "      <td>Organ</td>\n",
       "      <td>[vhead, _, _]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xmlab1/source.jpg</td>\n",
       "      <td>Does the picture contain kidney?</td>\n",
       "      <td>No</td>\n",
       "      <td>en</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>MRI</td>\n",
       "      <td>closed</td>\n",
       "      <td>vqa</td>\n",
       "      <td>Organ</td>\n",
       "      <td>[vhead, _, _]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>xmlab1/source.jpg</td>\n",
       "      <td>Does the picture contain spleen?</td>\n",
       "      <td>No</td>\n",
       "      <td>en</td>\n",
       "      <td>Abdomen</td>\n",
       "      <td>MRI</td>\n",
       "      <td>closed</td>\n",
       "      <td>vqa</td>\n",
       "      <td>Organ</td>\n",
       "      <td>[vhead, _, _]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>xmlab100/source.jpg</td>\n",
       "      <td>Does the picture contain liver?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>en</td>\n",
       "      <td>Lung</td>\n",
       "      <td>CT</td>\n",
       "      <td>closed</td>\n",
       "      <td>vqa</td>\n",
       "      <td>Organ</td>\n",
       "      <td>[vhead, _, _]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>xmlab100/source.jpg</td>\n",
       "      <td>Does the picture contain lung?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>en</td>\n",
       "      <td>Lung</td>\n",
       "      <td>CT</td>\n",
       "      <td>closed</td>\n",
       "      <td>vqa</td>\n",
       "      <td>Organ</td>\n",
       "      <td>[vhead, _, _]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   img_id             img_name                          question answer  \\\n",
       "0       1    xmlab1/source.jpg   Does the picture contain liver?    Yes   \n",
       "1       1    xmlab1/source.jpg  Does the picture contain kidney?     No   \n",
       "2       1    xmlab1/source.jpg  Does the picture contain spleen?     No   \n",
       "3     100  xmlab100/source.jpg   Does the picture contain liver?    Yes   \n",
       "4     100  xmlab100/source.jpg    Does the picture contain lung?    Yes   \n",
       "\n",
       "  q_lang location modality answer_type base_type content_type         triple  \\\n",
       "0     en  Abdomen      MRI      closed       vqa        Organ  [vhead, _, _]   \n",
       "1     en  Abdomen      MRI      closed       vqa        Organ  [vhead, _, _]   \n",
       "2     en  Abdomen      MRI      closed       vqa        Organ  [vhead, _, _]   \n",
       "3     en     Lung       CT      closed       vqa        Organ  [vhead, _, _]   \n",
       "4     en     Lung       CT      closed       vqa        Organ  [vhead, _, _]   \n",
       "\n",
       "   qid  \n",
       "0    3  \n",
       "1    4  \n",
       "2    5  \n",
       "3   11  \n",
       "4   12  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slake_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_llava_instruct_format(df, q_col='question', a_col='answer', img_col='img_name'):\n",
    "    converted = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        question = row[q_col].capitalize()\n",
    "        answer = row[a_col].lower().strip()\n",
    "        \n",
    "        # Parse out the options\n",
    "        if answer in ['yes', 'no']:\n",
    "            options = ['yes', 'no']\n",
    "        else:\n",
    "            if 'Is brain tumor white or gray relative to other tissues?' in question:\n",
    "                options = ['white', 'gray']\n",
    "            \n",
    "            elif 'Is the abnormality hyperdense or hypodense?' in question:\n",
    "                options = ['hyperdense', 'hypodense']\n",
    "\n",
    "            elif 'Is the brain enhancing tumor hyperdense or hypodense?' in question:\n",
    "                options = ['hyperdense', 'hypodense']\n",
    "\n",
    "            elif 'Is the brain non-enhancing tumor hyperdense or hypodense?' in question:\n",
    "                options = ['hyperdense', 'hypodense']\n",
    "\n",
    "            elif 'Is this a t1 weighted or t2 weighted mri image?' in question:\n",
    "                options = ['t1', 't2']\n",
    "\n",
    "            elif 'Which plane is the image scanned, transverse plane or coronal plane?' in question:\n",
    "                options = ['transverse plane', 'coronal plane']\n",
    "            \n",
    "            elif 'Which is bigger in this image,left lung or left kidney?' in question:\n",
    "                options = ['left lung', 'left kidney']\n",
    "\n",
    "            elif 'Which is bigger in this image,small bowel or kidney?' in question:\n",
    "                options = ['small bowel', 'kidney']\n",
    "\n",
    "            elif 'Which is smaller in this image,liver or spinal cord?' in question:\n",
    "                options = ['liver', 'spinal cord']\n",
    "\n",
    "            elif 'Which is smaller in this image,liver or right kidney?' in question:\n",
    "                options = ['liver', 'right kidney']\n",
    "\n",
    "            elif 'Which is smaller in this image,kidney or spinal cord?' in question:\n",
    "                options = ['kidney', 'spinal cord']\n",
    "\n",
    "            elif 'Which is smaller in this image, small bowel or right kidney?' in question:\n",
    "                options = ['small bowel', 'right kidney']\n",
    "\n",
    "            elif 'Which is the smallest in this image,spleen,left kidney or liver?' in question:\n",
    "                options = ['left kidney', 'liver']\n",
    "\n",
    "            elif 'Which is the smallest in this image, colon, left lung or liver?' in question:\n",
    "                options = ['colon', 'left lung', 'liver']\n",
    "\n",
    "            elif 'Which is bigger in this image, heart or right lung?' in question:\n",
    "                options = ['heart', 'right lung']\n",
    "\n",
    "            elif 'Which is smaller in this image, bladder or small bowel?' in question:\n",
    "                options = ['bladder', 'small bowel']\n",
    "\n",
    "            elif 'Which is bigger in this image, colon or small bowel?' in question:\n",
    "                options = ['colon', 'small bowel']\n",
    "\n",
    "            elif 'Which is bigger, left kidney or spleen ?' in question:\n",
    "                options = ['left kidney', 'spleen']\n",
    "\n",
    "            elif 'Where is the spleen in this image, right or lower right?' in question:\n",
    "                options = ['right', 'lower right']\n",
    "\n",
    "            elif 'Which is smaller in this image,liver or left lung?' in question:\n",
    "                options = ['liver', 'left lung']\n",
    "\n",
    "            elif 'Which is bigger in this image, small bowel or kidney?' in question:\n",
    "                options = ['small bowel', 'kidney']\n",
    "\n",
    "            elif 'Where is the left kidney in this image, right or lower right?' in question:\n",
    "                options = ['right', 'lower right']\n",
    "\n",
    "            elif 'Which is bigger in this image,small bowel or liver?' in question:\n",
    "                options = ['small bowel', 'liver']\n",
    "\n",
    "            elif 'Which is smaller in this image,spleen or right kidney?' in question:\n",
    "                options = ['spleen', 'right kidney']\n",
    "\n",
    "            elif 'Which is bigger in this image, rectum or small bowel?' in question:\n",
    "                options = ['rectum', 'small bowel']\n",
    "\n",
    "            elif 'Which is bigger in this image, kidney or spleen?' in question:\n",
    "                options = ['kidney', 'spleen']\n",
    "\n",
    "            elif 'Which is bigger in this image, kidney or spleen?' in question:\n",
    "                options = ['kidney', 'spleen']\n",
    "\n",
    "            elif 'Which is bigger in this image, small bowel or colon?' in question:\n",
    "                options = ['small bowel', 'colon']\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    option_str = question.split(',', 1)[1]\n",
    "                    option_str = option_str.strip('?')\n",
    "                    option_str = option_str.replace(',', ' ')\n",
    "                    option_str = option_str.replace('or', ' ')\n",
    "                    option_str = option_str.replace('  ', ' ')\n",
    "                    options = [o.lower() for o in option_str.split(' ')]\n",
    "                    options = [o for o in options if o != '']\n",
    "                except:\n",
    "                    options = []\n",
    "\n",
    "        if len(options) == 0:\n",
    "            print('No options found:')\n",
    "            print(f'Question: {question}')\n",
    "            print(f'Answer: {answer}\\n')\n",
    "            continue\n",
    "\n",
    "        if len(options) > 0 and answer not in options:\n",
    "            if answer in ['both', 'almost the same']:\n",
    "                options.append(answer)\n",
    "            else:\n",
    "                print('Mismatch between question and answer; excluded:')\n",
    "                print(f'Question: {question}')\n",
    "                print(f'Options: {options}')\n",
    "                print(f'Answer: {answer}\\n')\n",
    "                continue\n",
    "\n",
    "        # Build conversation\n",
    "        row_dict = dict(\n",
    "            id=osp.splitext(row[img_col])[0],\n",
    "            image=row[img_col],\n",
    "            conversations=[\n",
    "                {'from': 'human', 'value': f'{DEFAULT_IMAGE_TOKEN}\\n{question}'},\n",
    "                {'from': 'gpt', 'value': answer}\n",
    "            ],\n",
    "            options=options\n",
    "        )\n",
    "        converted.append(row_dict)\n",
    "\n",
    "    return converted\n",
    "\n",
    "slake_train_final = convert_to_llava_instruct_format(slake_train_df, img_col='img_name')\n",
    "slake_val_final = convert_to_llava_instruct_format(slake_val_df, img_col='img_name')\n",
    "slake_test_final = convert_to_llava_instruct_format(slake_test_df, img_col='img_name')\n",
    "\n",
    "slake_closed_dir = osp.join(slake_dir, 'closed')\n",
    "os.makedirs(slake_closed_dir, exist_ok=True)\n",
    "\n",
    "def write_jsonl(data, path):\n",
    "    with open(path, 'w') as fh:\n",
    "        for qa in data:\n",
    "            json.dump(qa, fh)\n",
    "            fh.write('\\n')\n",
    "\n",
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as fh:\n",
    "        for line in fh:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data\n",
    "\n",
    "write_jsonl(slake_train_final, osp.join(slake_closed_dir, 'train.jsonl'))\n",
    "write_jsonl(slake_val_final, osp.join(slake_closed_dir, 'val.jsonl'))\n",
    "write_jsonl(slake_test_final, osp.join(slake_closed_dir, 'test.jsonl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
