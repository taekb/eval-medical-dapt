{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing i2b2 Datasets\n",
    "\n",
    "We use the i2b2 challenge dataset from 2008 (Uzuner, 2009) for evaluating LLMs on comorbidity detection task based on de-identified discharge summaries.\n",
    "\n",
    "Preprocessing code was adapted from Arroyo et al. (2024): https://github.com/alceballosa/clin-robust/tree/master/preprocessing_notebooks.\n",
    "\n",
    "Before running the pipeline here, download and unzip the raw i2b2 dataset from the [Harvard DBMI portal](https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "import huggingface_hub\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "HF_CACHE_DIR = '/data/hf_models' # Modify as needed\n",
    "DATA_DIR = '/data' # Modify as needed\n",
    "N2C2_DIR = osp.join(DATA_DIR, 'n2c2')\n",
    "OBESITY_DIR = osp.join(N2C2_DIR, '2008 Obesity Challenge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for removing excessive whitespace\n",
    "def transform_string(s):\n",
    "    s = re.sub(r'(\\n\\s*|\\s*\\n)', '\\n', s)\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# Optional: Load a tokenizer of interest to filter out discharge notes longer than max token length\n",
    "hf_api_token = '' # Fill in with your own token\n",
    "huggingface_hub.login(token=hf_api_token)\n",
    "\n",
    "# NOTE: We use Llama-2 to filter out long clinical notes, as the Llama-2 tokenizer has the smallest vocabulary size among the LLMs we evaluate.\n",
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=HF_CACHE_DIR)\n",
    "\n",
    "def longer_than_max_length(note, tokenizer=tokenizer, max_length=3000, verbose=False):\n",
    "    '''Checks if a given note is longer than the max token length.'''\n",
    "\n",
    "    token_length = tokenizer(note, return_tensors='pt')['input_ids'].shape[-1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Length: {token_length}')\n",
    "\n",
    "    return token_length > max_length\n",
    "\n",
    "def filter_by_len(data_list, max_length=3000):\n",
    "    '''Filters out samples with clinical notes longer than max_length.'''\n",
    "\n",
    "    n_short = 0\n",
    "    n_long = 0\n",
    "    short_idxs = []\n",
    "\n",
    "    for i, sample in enumerate(data_list):\n",
    "        if longer_than_max_length(sample['text'], max_length=max_length):\n",
    "            n_long += 1\n",
    "        else:\n",
    "            n_short += 1\n",
    "            short_idxs.append(i)\n",
    "\n",
    "    assert(n_short + n_long == len(data_list))\n",
    "    print(f'Included: {n_short}, Excluded: {n_long}')\n",
    "    data_list = [data_list[i] for i in short_idxs]\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Arroyo et al. (2024), we focus on 4 prediction tasks for predicting asthma, CAD, diabetes, and obesity, using the \"intuitive\" annotations on all clinical notes. See https://www.i2b2.org/NLP/Obesity/Documentation.php for more details on how the labels are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching labels for \"Asthma\": 100%|██████████| 572/572 [00:00<00:00, 15315.50it/s]\n",
      "Fetching labels for \"CAD\": 100%|██████████| 552/552 [00:00<00:00, 15436.89it/s]\n",
      "Fetching labels for \"Diabetes\": 100%|██████████| 572/572 [00:00<00:00, 14832.87it/s]\n",
      "Fetching labels for \"Obesity\": 100%|██████████| 554/554 [00:00<00:00, 15423.72it/s]\n"
     ]
    }
   ],
   "source": [
    "train_record_file = osp.join(OBESITY_DIR, 'obesity_patient_records_training.xml')\n",
    "train_tree = ET.parse(train_record_file)\n",
    "train_root = train_tree.getroot()\n",
    "\n",
    "train2_record_file = osp.join(OBESITY_DIR, 'obesity_patient_records_training2.xml')\n",
    "train2_tree = ET.parse(train2_record_file)\n",
    "train2_root = train2_tree.getroot()\n",
    "\n",
    "# Merge the two training document files\n",
    "for child in train2_root:\n",
    "    train_root.append(child)\n",
    "\n",
    "# Labels (\"intuitive\")\n",
    "train_annotation_file = osp.join(OBESITY_DIR, 'obesity_standoff_intuitive_annotations_training.xml')\n",
    "train_annotation_tree = ET.parse(train_annotation_file)\n",
    "train_annotation_root = train_annotation_tree.getroot()\n",
    "\n",
    "train_set = []\n",
    "for docs in train_root:\n",
    "    for doc in docs:\n",
    "        doc_id = doc.attrib[\"id\"]\n",
    "        for text in doc:\n",
    "            doc_text = text.text\n",
    "            #doc_text = transform_string(text.text) # For some reason, keeping the whitespace seems to work better\n",
    "            train_set.append({\"id\": doc_id, \"text\": doc_text})\n",
    "\n",
    "target_diseases = ['Asthma', 'CAD', 'Diabetes', 'Obesity']\n",
    "disease_to_train_set = {}\n",
    "\n",
    "# Optional: Set to False to keep all samples\n",
    "apply_filter = False\n",
    "\n",
    "for diseaseset in train_annotation_root:\n",
    "    judgment_type = diseaseset.attrib[\"source\"]\n",
    "    for disease in diseaseset:\n",
    "        disease_train_set = []\n",
    "        disease_name = disease.attrib[\"name\"]\n",
    "\n",
    "        if disease_name not in target_diseases:\n",
    "            continue\n",
    "        \n",
    "        pbar = tqdm(disease, desc=f'Fetching labels for \"{disease_name}\"')\n",
    "        for doc in pbar:\n",
    "            doc_id = doc.attrib['id']\n",
    "            doc_judgment = doc.attrib['judgment'] # Y/N/Q\n",
    "\n",
    "            # Add label to matching clinical note\n",
    "            for i, sample in enumerate(train_set):\n",
    "                if sample['id'] == doc_id:\n",
    "                    feature_name = f\"{disease_name.lower()}\"\n",
    "                    assert feature_name not in sample.keys(), f\"Feature {feature_name} already exists!\"\n",
    "                    disease_train_set.append(sample | {'label': doc_judgment})\n",
    "\n",
    "        if apply_filter:\n",
    "            disease_train_set = filter_by_len(disease_train_set)\n",
    "\n",
    "        disease_to_train_set[disease_name.lower()] = Dataset.from_list(disease_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching labels for \"Asthma\": 100%|██████████| 471/471 [00:00<00:00, 23657.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included: 357, Excluded: 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching labels for \"CAD\": 100%|██████████| 458/458 [00:00<00:00, 23768.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included: 335, Excluded: 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching labels for \"Diabetes\": 100%|██████████| 479/479 [00:00<00:00, 24047.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included: 358, Excluded: 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching labels for \"Obesity\": 100%|██████████| 447/447 [00:00<00:00, 23994.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included: 334, Excluded: 113\n"
     ]
    }
   ],
   "source": [
    "test_record_file = osp.join(OBESITY_DIR, 'obesity_patient_records_test.xml')\n",
    "test_tree = ET.parse(test_record_file)\n",
    "test_root = test_tree.getroot()\n",
    "\n",
    "test_annotation_file = osp.join(OBESITY_DIR, 'obesity_standoff_annotations_test_intuitive.xml')\n",
    "test_annotation_tree = ET.parse(test_annotation_file)\n",
    "test_annotation_root = test_annotation_tree.getroot()\n",
    "\n",
    "test_set = []\n",
    "for docs in test_root:\n",
    "    for doc in docs:\n",
    "        doc_id = doc.attrib[\"id\"]\n",
    "        for text in doc:\n",
    "            doc_text = text.text\n",
    "            test_set.append({\"id\": doc_id, \"text\": doc_text})\n",
    "    \n",
    "target_diseases = ['Asthma', 'CAD', 'Diabetes', 'Obesity']\n",
    "disease_to_test_set = {}\n",
    "\n",
    "# Optional: Set to False to keep all samples\n",
    "apply_filter = True\n",
    "\n",
    "for diseaseset in test_annotation_root:\n",
    "    judgment_type = diseaseset.attrib[\"source\"]\n",
    "    for disease in diseaseset:\n",
    "        disease_test_set = []\n",
    "        disease_name = disease.attrib[\"name\"]\n",
    "        \n",
    "        if disease_name not in target_diseases:\n",
    "            continue\n",
    "        \n",
    "        pbar = tqdm(disease, desc=f'Fetching labels for \"{disease_name}\"')\n",
    "        for doc in pbar:\n",
    "            doc_id = doc.attrib[\"id\"]\n",
    "            doc_judgment = doc.attrib[\"judgment\"] # Y/N/Q\n",
    "            \n",
    "            # Add label to matching clinical note\n",
    "            for i, sample in enumerate(test_set):\n",
    "                if sample['id'] == doc_id:\n",
    "                    feature_name = f\"{disease_name.lower()}\"\n",
    "                    assert feature_name not in sample.keys(), f\"Feature {feature_name} already exists!\"\n",
    "                    disease_test_set.append(sample | {'label': doc_judgment})\n",
    "\n",
    "        if apply_filter:\n",
    "            disease_test_set = filter_by_len(disease_test_set)\n",
    "\n",
    "        disease_to_test_set[disease_name.lower()] = Dataset.from_list(disease_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disease_name in target_diseases:\n",
    "    save_dir = osp.join(DATA_DIR, f'n2c2_2008-obesity_{disease_name.lower()}')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'Saving comorbidity dataset for \"{disease_name.lower()}\"...')\n",
    "    disease_dataset = DatasetDict(dict(\n",
    "        train=disease_to_train_set[disease_name.lower()],\n",
    "        test=disease_to_test_set[disease_name.lower()]\n",
    "    ))\n",
    "    disease_dataset.save_to_disk(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asthma: Counter({'N': 309, 'Y': 48})\n",
      "CAD: Counter({'Y': 192, 'N': 142, 'Q': 1})\n",
      "Diabetes: Counter({'Y': 227, 'N': 131})\n",
      "Obesity: Counter({'N': 191, 'Y': 143})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Class label counts\n",
    "for disease_name in target_diseases:\n",
    "    disease_labels = disease_to_test_set[disease_name.lower()]['label']\n",
    "    counts = Counter(disease_labels)\n",
    "    print(f'{disease_name}: {counts}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
